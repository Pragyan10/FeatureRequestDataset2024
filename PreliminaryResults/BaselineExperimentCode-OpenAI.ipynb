{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rambodparsi/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "file_name = '10InitialFeatureRequests.csv'\n",
    "df = pd.read_csv(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the vanilla_prompt\n",
    "vanilla_prompt = \"Identify all categories of ambiguity in the following text. The ambiguity categories are lexical, syntactic, semantic, pragmatic, and vagueness.\"\n",
    "# vanilla_prompt = \"You are a software analyst specializing in ambiguity detection. Your task is to identify all the ambiguities in the following text. The ambiguity categories are lexical, syntactic, semantic, pragmatic, and vagueness.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the new column 'prompt'\n",
    "df['prompt'] = vanilla_prompt + ' \"' + df['FeatureRequestText'] + '\"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identify all categories of ambiguity in the following text. The ambiguity categories are lexical, syntactic, semantic, pragmatic, and vagueness. \"Request Title: Reloading gets you to latest toot, losing hability to read oldest unread first - Request Description: Maybe this is a feature, not a bug of the app, but certainly it is what keeps me using Fedilab for now. With Fedilab, the loading of new toots keeps my current position on the timeline, so I can read things in their natural order. In official Mastodon app, as it takes you to the top, final toot, it forces you to scroll back down to whatever was the last toot you saw.\"\n"
     ]
    }
   ],
   "source": [
    "print(df.iloc[0,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Prompt column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated dataset with prompt and saved as 10InitialFeatureRequests.csv\n"
     ]
    }
   ],
   "source": [
    "# Extract the original file name without the extension\n",
    "base_name = os.path.splitext(file_name)[0]\n",
    "\n",
    "# Create the new file name\n",
    "updated_file_name = f\"{base_name}.csv\"\n",
    "\n",
    "# Save the updated dataset back to a CSV file\n",
    "df.to_csv(updated_file_name, index=False)\n",
    "\n",
    "print(f\"Updated dataset with prompt and saved as {updated_file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenAI API key\n",
    "# add your openai api key here and save it as openai.api_key\n",
    "\n",
    "def get_gpt_response(prompt):\n",
    "    response = openai.Completion.create(\n",
    "        model=\"gpt-3.5-turbo-instruct\",\n",
    "        prompt=prompt,\n",
    "        max_tokens=2000,\n",
    "    )\n",
    "    return response.choices[0].text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get responses from GPT-3.5-turbo-instruct and store them in a new column\n",
    "df['gpt_3_5_turbo_instruct_response'] = df['prompt'].apply(get_gpt_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Lexical: The use of the term \"toot\" may be unfamiliar or ambiguous to some readers.\n",
      "2. Semantic: The phrase \"boost notifications\" could refer to either notifications of a boosted toot or notifications for boosts given to the user's toot.\n",
      "3. Syntactic: The phrase \"got quite a few favourites and boosts\" could be interpreted as the user receiving favorites and boosts, or the toot itself receiving them.\n",
      "4. Pragmatic: The request description does not specify if this feature would apply to all types of notifications or only certain ones.\n",
      "5. Vagueness: The phrase \"a few\" in the request description is subjective and does not give a specific number.\n"
     ]
    }
   ],
   "source": [
    "print(df['gpt_3_5_turbo_instruct_response'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated dataset with GPT responses saved as 10InitialFeatureRequestsresponse.csv\n"
     ]
    }
   ],
   "source": [
    "updated_file_name = f\"{base_name}response.csv\"\n",
    "\n",
    "# Save the updated dataset back to a CSV file\n",
    "df.to_csv(updated_file_name, index=False)\n",
    "\n",
    "print(f\"Updated dataset with GPT responses saved as {updated_file_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
